# FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection

<p align="center">
    <a href="https://aaai.org/aaai-26/">ðŸ§  AAAI 2026</a> â€¢ 
    <a href="https://arxiv.org/abs/2502.15488">ðŸ“„ Paper (arXiv)</a> â€¢ 
    <a href="#citation">ðŸ“š Citation</a>
</p>

<p align="center">
    <img src="docs/assets/fqpetr_banner.png" width="70%">
</p>

---

### ðŸš§ Coming Soon

This repository will provide the **official implementation** of our AAAI 2026 paper:

> **FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection**  
> *Jiangyong YuÂ¹, Changyong ShuÂ¹*, Sifan ZhouÂ², Zichen YuÂ³, Xing HuÂ¹, Yan ChenÂ¹, Dawei YangÂ¹*  
> Â¹ Houmo AI, Â² Southeast University, Â³ Dalian University of Technology

Code, models, and documentation are being prepared.  
---

## ðŸ“‹ Overview

**FQ-PETR** introduces the first fully-quantized framework for **multi-view 3D object detection** based on PETR.  
It features:

- ðŸ”¸ **Quantization-friendly position embedding transformation**  
- ðŸ”¸ **End-to-end INT8 pipeline** preserving detection accuracy  
- ðŸ”¸ **DULUT design** that abstracts nonlinear LUT allocation as a differentiable function and replaces it with a linear LUT, requiring **no dedicated hardware support** 
